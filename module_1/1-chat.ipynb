{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef597741-3211-4ecc-92f7-f58023ee237e",
   "metadata": {},
   "source": [
    "# AI Agents Course\n",
    "\n",
    "This serios of notebooks is summarised and slightly adopted course [Introduction to LangGraph](https://academy.langchain.com/courses/intro-to-langgraph) by LangChain Academy.\n",
    "\n",
    "## Setup\n",
    "Best way to install is to use the `requirements.txt` file. In the root folder of the repository, run:\n",
    "``` bash\n",
    "python -m venv .venv\n",
    "source .venv/bin/activate\n",
    "pip install -r requirements.txt\n",
    "\n",
    "```\n",
    "\n",
    "# OPENAI_API_KEY \n",
    "Obtain your API key from https://platform.openai.com/api-keys. And set it as an environment variable:\n",
    "\n",
    "``` bash\n",
    "export OPENAI_API_KEY=<your-api-key>\n",
    "```\n",
    "\n",
    "\n",
    "## Chat models\n",
    "\n",
    "In this course, we'll be using [Chat Models](https://python.langchain.com/v0.2/docs/concepts/#chat-models), which do a few things take a sequence of messages as inputs and return chat messages as outputs. LangChain does not host any Chat Models, rather we rely on third party integrations. [Here](https://python.langchain.com/v0.2/docs/integrations/chat/) is a list of 3rd party chat model integrations within LangChain! By default, the course will use [ChatOpenAI](https://python.langchain.com/v0.2/docs/integrations/chat/openai/) because it is both popular and performant.\n",
    "\n",
    "\n",
    "Let's check that your `OPENAI_API_KEY` is set and, if not, you will be asked to enter it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2a15227",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T14:55:24.394189Z",
     "start_time": "2025-03-15T14:55:12.218842Z"
    }
   },
   "outputs": [],
   "source": [
    "import os, getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a326f35b",
   "metadata": {},
   "source": [
    "# Ok, we are set. Let's begin!\n",
    "\n",
    "There are [a few standard parameters](https://python.langchain.com/v0.2/docs/concepts/#chat-models) that we can set with chat models. Two of the most common are:\n",
    "\n",
    "* `model`: the name of the model\n",
    "* `temperature`: the sampling temperature\n",
    "\n",
    "`Temperature` controls the randomness or creativity of the model's output where low temperature (close to 0) is more deterministic and focused outputs. This is good for tasks requiring accuracy or factual responses. High temperature (close to 1) is good for creative tasks or generating varied responses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e19a54d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T14:55:24.437327Z",
     "start_time": "2025-03-15T14:55:24.403302Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "gpt4o_chat = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "gpt35_chat = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28450d1b",
   "metadata": {},
   "source": [
    "Chat models in LangChain have a number of [default methods](https://python.langchain.com/v0.2/docs/concepts/#runnable-interface). For the most part, we'll be using:\n",
    "\n",
    "* `stream`: stream back chunks of the response\n",
    "* `invoke`: call the chain on an input\n",
    "\n",
    "And, as mentioned, chat models take [messages](https://python.langchain.com/v0.2/docs/concepts/#messages) as input. Messages have a role (that describes who is saying the message) and a content property. We'll be talking a lot more about this later, but here let's just show the basics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1280e1b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T14:55:25.317829Z",
     "start_time": "2025-03-15T14:55:24.456878Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 11, 'total_tokens': 21, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_eb9dce56a8', 'finish_reason': 'stop', 'logprobs': None}, id='run-d3201767-237c-4530-b41c-8888ad8fb1de-0', usage_metadata={'input_tokens': 11, 'output_tokens': 10, 'total_tokens': 21, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "import json\n",
    "\n",
    "# Create a message\n",
    "msg = HumanMessage(content=\"Hello world\", name=\"Lance\")\n",
    "# Q: What if we don't set the name? Why the name is needed?\n",
    "#    Check what is the answer to the HumanMessage with content=\"What is my name?\"\n",
    "#    Provide a running example of a dialog that would be different if the name is not set.\n",
    "\n",
    "# Message list\n",
    "messages = [msg]\n",
    "\n",
    "# Invoke the model with a list of messages \n",
    "response = gpt4o_chat.invoke(messages)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac73e4c",
   "metadata": {},
   "source": [
    "We get an `AIMessage` response. Also, note that we can just invoke a chat model with a string. When a string is passed in as input, it is converted to a `HumanMessage` and then passed to the underlying model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f27c6c9a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T14:55:26.025934Z",
     "start_time": "2025-03-15T14:55:25.323815Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 9, 'total_tokens': 19, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_eb9dce56a8', 'finish_reason': 'stop', 'logprobs': None}, id='run-8c40c83d-cc75-4bfe-b09b-38f9f99a1561-0', usage_metadata={'input_tokens': 9, 'output_tokens': 10, 'total_tokens': 19, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt4o_chat.invoke(\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdc2f0ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T14:55:26.543847Z",
     "start_time": "2025-03-15T14:55:26.036603Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 9, 'total_tokens': 19, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-3a170093-dab1-4f0a-9f73-65ba0fcceb83-0', usage_metadata={'input_tokens': 9, 'output_tokens': 10, 'total_tokens': 19, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt35_chat.invoke(\"hello world\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582c0e5a",
   "metadata": {},
   "source": [
    "The interface is consistent across all chat models and models are typically initialized once at the start up each notebooks. \n",
    "\n",
    "So, you can easily switch between models without changing the downstream code if you have strong preference for another provider.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38d4e94cd6a4af0",
   "metadata": {},
   "source": [
    "## Messages\n",
    "\n",
    "Chat models can use [`messages`](https://python.langchain.com/v0.2/docs/concepts/#messages), which capture different roles within a conversation.\n",
    "\n",
    "LangChain supports various message types, including `HumanMessage`, `AIMessage`, `SystemMessage`, and `ToolMessage`.\n",
    "\n",
    "These represent a message from the user, from chat model, for the chat model to instruct behavior, and from a tool call.\n",
    "\n",
    "Let's create a list of messages.\n",
    "\n",
    "Each message can be supplied with a few things:\n",
    "\n",
    "* `content` - content of the message\n",
    "* `name` - optionally, a message author\n",
    "* `response_metadata` - optionally, a dict of metadata (e.g., often populated by model provider for `AIMessages`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bafd7d5d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T14:55:26.561783Z",
     "start_time": "2025-03-15T14:55:26.557873Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: Model\n",
      "\n",
      "So you said you were researching ocean mammals?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "Name: Lance\n",
      "\n",
      "Yes, that's right.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: Model\n",
      "\n",
      "Great, what would you like to learn about.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "Name: Lance\n",
      "\n",
      "I want to learn about the best place to see turtles in the Cyprus.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "messages = [AIMessage(content=f\"So you said you were researching ocean mammals?\", name=\"Model\")]\n",
    "messages.append(HumanMessage(content=f\"Yes, that's right.\", name=\"Lance\"))\n",
    "messages.append(AIMessage(content=f\"Great, what would you like to learn about.\", name=\"Model\"))\n",
    "messages.append(HumanMessage(content=f\"I want to learn about the best place to see turtles in the Cyprus.\", name=\"Lance\"))\n",
    "\n",
    "for m in messages:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c95221da5e33fa8f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T14:55:31.265847Z",
     "start_time": "2025-03-15T14:55:26.579959Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.messages.ai.AIMessage"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = gpt4o_chat.invoke(messages)\n",
    "type(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45804516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cyprus is a fantastic place to observe sea turtles, particularly the loggerhead (Caretta caretta) and green turtles (Chelonia mydas). Here are some of the best places to see them:\n",
      "\n",
      "1. **Lara Beach**: Located in the Akamas Peninsula, Lara Beach is one of the most important nesting sites for both loggerhead and green turtles in Cyprus. The beach is part of a protected area, and there are conservation efforts in place to ensure the safety of the turtles and their nests. Visiting during the nesting season, which typically runs from June to September, offers the best chance to see turtles.\n",
      "\n",
      "2. **Alagadi Beach**: Situated near Kyrenia in Northern Cyprus, Alagadi Beach is another significant nesting site. The Society for the Protection of Turtles (SPOT) conducts conservation work here, and they often organize guided tours during the nesting season to educate visitors about the turtles and their conservation.\n",
      "\n",
      "3. **Polis and Latchi Beaches**: These beaches, located in the Paphos District, are also known for turtle nesting. While not as prominent as Lara Beach, they still offer opportunities to see turtles, especially if you visit during the early morning or late evening when turtles are more active.\n",
      "\n",
      "4. **Akamas Peninsula**: Beyond Lara Beach, the entire Akamas Peninsula is a protected area with rich biodiversity, including sea turtles. Exploring the region can provide additional opportunities to spot turtles in their natural habitat.\n",
      "\n",
      "When visiting these sites, it's important to follow local guidelines and respect the conservation efforts in place to protect these magnificent creatures. Avoid disturbing nests, keep a respectful distance from turtles, and follow any instructions provided by local conservation groups.\n"
     ]
    }
   ],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60120f46e4142da7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T14:57:23.775421Z",
     "start_time": "2025-03-15T14:57:23.773011Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"token_usage\": {\n",
      "    \"completion_tokens\": 342,\n",
      "    \"prompt_tokens\": 66,\n",
      "    \"total_tokens\": 408,\n",
      "    \"completion_tokens_details\": {\n",
      "      \"accepted_prediction_tokens\": 0,\n",
      "      \"audio_tokens\": 0,\n",
      "      \"reasoning_tokens\": 0,\n",
      "      \"rejected_prediction_tokens\": 0\n",
      "    },\n",
      "    \"prompt_tokens_details\": {\n",
      "      \"audio_tokens\": 0,\n",
      "      \"cached_tokens\": 0\n",
      "    }\n",
      "  },\n",
      "  \"model_name\": \"gpt-4o-2024-08-06\",\n",
      "  \"system_fingerprint\": \"fp_f9f4fb6dbf\",\n",
      "  \"finish_reason\": \"stop\",\n",
      "  \"logprobs\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(result.response_metadata, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88332e4f",
   "metadata": {},
   "source": [
    "# Task: \n",
    "1. Find the answer to all questions in the notebook (e.g. Q: what if we don't set the output type?)\n",
    "2. There is also a SystemMessage. What is the difference between SystemMessage and HumanMessage?\n",
    "\n",
    "   Provide an example of a dialog that would be different if we use SystemMessage instead of HumanMessage. \n",
    "   \n",
    "   Use temperature=0 to make the model deterministic.\n",
    "\n",
    "3. Can the dialog have more than one SystemMessage?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a4300c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
