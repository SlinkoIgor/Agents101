{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory Collection Schema\n",
    "Sometimes we want to save memories to a [collection](https://docs.google.com/presentation/d/181mvjlgsnxudQI6S3ritg9sooNyu4AcLLFH1UK0kIuk/edit#slide=id.g30eb3c8cf10_0_200) rather than single profile. \n",
    "\n",
    "We'll also show how to use [Trustcall](https://github.com/hinthornw/trustcall) to update this collection. \n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T20:29:14.951548Z",
     "start_time": "2025-03-23T20:29:12.889881Z"
    }
   },
   "source": [
    "%%capture --no-stderr\n",
    "%pip install -U langchain_openai langgraph trustcall langchain_core"
   ],
   "outputs": [],
   "execution_count": 99
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T20:29:14.959065Z",
     "start_time": "2025-03-23T20:29:14.956093Z"
    }
   },
   "source": [
    "import os, getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    # Check if the variable is set in the OS environment\n",
    "    env_value = os.environ.get(var)\n",
    "    if not env_value:\n",
    "        # If not set, prompt the user for input\n",
    "        env_value = getpass.getpass(f\"{var}: \")\n",
    "    \n",
    "    # Set the environment variable for the current process\n",
    "    os.environ[var] = env_value"
   ],
   "outputs": [],
   "execution_count": 100
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T20:29:20.979710Z",
     "start_time": "2025-03-23T20:29:20.977396Z"
    }
   },
   "source": [
    "_set_env(\"OPENAI_API_KEY\")"
   ],
   "outputs": [],
   "execution_count": 101
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Defining a collection schema\n",
    "\n",
    "Instead of storing user information in a fixed profile structure, we'll create a flexible collection schema to store memories about user interactions.\n",
    "\n",
    "Each memory will be stored as a separate entry with a single `content` field for the main information we want to remember\n",
    "\n",
    "This approach allows us to build an open-ended collection of memories that can grow and change as we learn more about the user.\n",
    "\n",
    "We can define a collection schema as a [Pydantic](https://docs.pydantic.dev/latest/) object."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T20:29:43.282447Z",
     "start_time": "2025-03-23T20:29:43.278179Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Memory(BaseModel):\n",
    "    content: str = Field(description=\"The main content of the memory. For example: User expressed interest in learning about French.\")\n",
    "\n",
    "class MemoryCollection(BaseModel):\n",
    "    memories: list[Memory] = Field(description=\"A list of memories about the user.\")"
   ],
   "outputs": [],
   "execution_count": 102
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structured output\n",
    "We can used LangChain's chat model [chat model](https://python.langchain.com/docs/concepts/chat_models/) interface's [`with_structured_output`](https://python.langchain.com/docs/concepts/structured_outputs/#recommended-usage) method to enforce structured output."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T20:29:46.933074Z",
     "start_time": "2025-03-23T20:29:45.287892Z"
    }
   },
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Initialize the model\n",
    "model = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "# Bind schema to model\n",
    "model_with_structure = model.with_structured_output(MemoryCollection)\n",
    "\n",
    "# Invoke the model to produce structured output that matches the schema\n",
    "memory_collection = model_with_structure.invoke([HumanMessage(\"My name is Lance. I like to bike.\")]) # Q: How it works?\n",
    "memory_collection.memories"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Memory(content=\"User's name is Lance.\"),\n",
       " Memory(content='User likes to bike.')]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 103
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `model_dump()` to serialize a Pydantic model instance into a Python dictionary."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T20:29:50.337937Z",
     "start_time": "2025-03-23T20:29:50.334770Z"
    }
   },
   "source": [
    "memory_collection.memories[0].model_dump()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': \"User's name is Lance.\"}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 104
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Updating collection schema\n",
    "\n",
    "We want to *update existing memories* with *new memories* in the collection.\n",
    "Now we'll show that [Trustcall](https://github.com/hinthornw/trustcall) can be also used to update a collection.\n",
    "This enables both addition of new memories as well as [updating existing memories in the collection](https://github.com/hinthornw/trustcall?tab=readme-ov-file#simultanous-updates--insertions\n",
    ")."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T20:30:03.481281Z",
     "start_time": "2025-03-23T20:30:03.456507Z"
    }
   },
   "source": [
    "from trustcall import create_extractor\n",
    "\n",
    "# Create the extractor which will parse structured output from model to object\n",
    "trustcall_extractor = create_extractor(\n",
    "    model,\n",
    "    tools=[Memory],\n",
    "    tool_choice=\"Memory\",\n",
    "    enable_inserts=True, # to allow the extractor to insert new memories to the collection\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 105
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T20:30:09.108176Z",
     "start_time": "2025-03-23T20:30:07.935625Z"
    }
   },
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "# Instruction\n",
    "instruction = \"\"\"Extract memories from the following conversation:\"\"\"\n",
    "\n",
    "# Conversation\n",
    "conversation = [HumanMessage(content=\"Hi, I'm Lance.\"), \n",
    "                AIMessage(content=\"Nice to meet you, Lance.\"), \n",
    "                HumanMessage(content=\"This morning I had a nice bike ride in San Francisco.\")]\n",
    "\n",
    "# Invoke the extractor\n",
    "result = trustcall_extractor.invoke({\"messages\": [SystemMessage(content=instruction)] + conversation})"
   ],
   "outputs": [],
   "execution_count": 106
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T20:30:09.609825Z",
     "start_time": "2025-03-23T20:30:09.606504Z"
    }
   },
   "source": [
    "# Messages contain the tool calls\n",
    "for m in result[\"messages\"]:\n",
    "    m.pretty_print()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001B[1m Ai Message \u001B[0m==================================\n",
      "Tool Calls:\n",
      "  Memory (call_Hh02B7oiKQs3nidz1boewy7B)\n",
      " Call ID: call_Hh02B7oiKQs3nidz1boewy7B\n",
      "  Args:\n",
      "    content: Lance had a nice bike ride in San Francisco this morning.\n"
     ]
    }
   ],
   "execution_count": 107
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T20:30:10.820383Z",
     "start_time": "2025-03-23T20:30:10.817323Z"
    }
   },
   "source": [
    "# Responses contain the memories that adhere to the schema\n",
    "for m in result[\"responses\"]: \n",
    "    print(m)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Lance had a nice bike ride in San Francisco this morning.'\n"
     ]
    }
   ],
   "execution_count": 108
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T20:30:11.612982Z",
     "start_time": "2025-03-23T20:30:11.609667Z"
    }
   },
   "source": [
    "# Metadata contains the tool call  \n",
    "for m in result[\"response_metadata\"]: \n",
    "    print(m)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'call_Hh02B7oiKQs3nidz1boewy7B'}\n"
     ]
    }
   ],
   "execution_count": 109
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T20:30:47.870678Z",
     "start_time": "2025-03-23T20:30:47.865917Z"
    }
   },
   "source": [
    "# Update the conversation\n",
    "updated_conversation = [AIMessage(content=\"That's great, did you do after?\"), \n",
    "                        HumanMessage(content=\"I went to Tartine and ate a croissant.\"),                        \n",
    "                        AIMessage(content=\"What else is on your mind?\"),\n",
    "                        HumanMessage(content=\"I was thinking about my Japan, and going back this winter!\"),]\n",
    "\n",
    "# Update the instruction\n",
    "system_msg = \"\"\"Update existing memories and create new ones based on the following conversation:\"\"\"\n",
    "\n",
    "# We'll save existing memories, giving them an ID, key (tool name), and value\n",
    "tool_name = \"Memory\"\n",
    "existing_memories = [(str(i), tool_name, memory.model_dump()) for i, memory in enumerate(result[\"responses\"])] if result[\"responses\"] else None\n",
    "existing_memories"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('0',\n",
       "  'Memory',\n",
       "  {'content': 'Lance had a nice bike ride in San Francisco this morning.'})]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 110
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T20:30:52.045100Z",
     "start_time": "2025-03-23T20:30:50.403095Z"
    }
   },
   "source": [
    "# Invoke the extractor with our updated conversation and existing memories\n",
    "result = trustcall_extractor.invoke({\"messages\": updated_conversation, \n",
    "                                     \"existing\": existing_memories})"
   ],
   "outputs": [],
   "execution_count": 111
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T20:30:52.588848Z",
     "start_time": "2025-03-23T20:30:52.585964Z"
    }
   },
   "source": [
    "# Messages from the model indicate two tool calls were made\n",
    "for m in result[\"messages\"]:\n",
    "    m.pretty_print()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001B[1m Ai Message \u001B[0m==================================\n",
      "Tool Calls:\n",
      "  Memory (call_24K4Wl3z3Hc78kPG4WOywdJG)\n",
      " Call ID: call_24K4Wl3z3Hc78kPG4WOywdJG\n",
      "  Args:\n",
      "    content: Lance had a nice bike ride in San Francisco this morning.\n",
      "    -: User was thinking about Japan and going back this winter.\n",
      "  Memory (call_byQou4VdqPrZLz2tVwz5Cyfz)\n",
      " Call ID: call_byQou4VdqPrZLz2tVwz5Cyfz\n",
      "  Args:\n",
      "    content: User went to Tartine and ate a croissant.\n",
      "  Memory (call_bnvxr8LhcjGKex6E3G2f9ttQ)\n",
      " Call ID: call_bnvxr8LhcjGKex6E3G2f9ttQ\n",
      "  Args:\n",
      "    content: User was thinking about Japan and going back this winter.\n"
     ]
    }
   ],
   "execution_count": 112
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T20:30:57.579080Z",
     "start_time": "2025-03-23T20:30:57.576513Z"
    }
   },
   "source": [
    "# Responses contain the memories that adhere to the schema\n",
    "for m in result[\"responses\"]: \n",
    "    print(m)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Lance had a nice bike ride in San Francisco this morning.'\n",
      "content='User went to Tartine and ate a croissant.'\n",
      "content='User was thinking about Japan and going back this winter.'\n"
     ]
    }
   ],
   "execution_count": 113
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T20:31:01.617310Z",
     "start_time": "2025-03-23T20:31:01.614534Z"
    }
   },
   "source": [
    "# Metadata contains the tool call  \n",
    "for m in result[\"response_metadata\"]: \n",
    "    print(m)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'call_24K4Wl3z3Hc78kPG4WOywdJG', 'json_doc_id': '0'}\n",
      "{'id': 'call_byQou4VdqPrZLz2tVwz5Cyfz'}\n",
      "{'id': 'call_bnvxr8LhcjGKex6E3G2f9ttQ'}\n"
     ]
    }
   ],
   "execution_count": 114
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### In memory key-value store\n",
    "Save dictionary representation of each memory to the store."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T20:31:09.038556Z",
     "start_time": "2025-03-23T20:31:09.035068Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import uuid\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "\n",
    "# Initialize the in-memory store where we will store memories\n",
    "in_memory_store = InMemoryStore() # Q: What is the good replacement for InMemoryStore?\n",
    "\n",
    "# Namespace for the memory to save\n",
    "user_id = \"1\"\n",
    "namespace_for_memory = (user_id, \"memories\") # Q: Why we need namespace?\n",
    "\n",
    "# Save a memory to namespace as key and value\n",
    "key = str(uuid.uuid4())\n",
    "value = memory_collection.memories[0].model_dump()\n",
    "in_memory_store.put(namespace_for_memory, key, value)\n",
    "\n",
    "key = str(uuid.uuid4())\n",
    "value = memory_collection.memories[1].model_dump()\n",
    "in_memory_store.put(namespace_for_memory, key, value)\n",
    "\n",
    "# Q: If I reload notebook this storage will stay?"
   ],
   "outputs": [],
   "execution_count": 115
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Search for memories in the store."
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T20:31:10.993790Z",
     "start_time": "2025-03-23T20:31:10.990716Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Search\n",
    "for m in in_memory_store.search(namespace_for_memory): # Q: What are the possible arguments for search?\n",
    "    print(m.dict())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'namespace': ['1', 'memories'], 'key': '544ba667-ad23-47bc-8928-148c1771754f', 'value': {'content': \"User's name is Lance.\"}, 'created_at': '2025-03-23T20:31:09.037052+00:00', 'updated_at': '2025-03-23T20:31:09.037053+00:00', 'score': None}\n",
      "{'namespace': ['1', 'memories'], 'key': '1a2fdba5-c052-45de-8d10-4ee2691e3cff', 'value': {'content': 'User likes to bike.'}, 'created_at': '2025-03-23T20:31:09.037116+00:00', 'updated_at': '2025-03-23T20:31:09.037117+00:00', 'score': None}\n"
     ]
    }
   ],
   "execution_count": 116
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T20:31:13.751323Z",
     "start_time": "2025-03-23T20:31:12.312134Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.embeddings import init_embeddings\n",
    "\n",
    "in_memory_embed_store = InMemoryStore(index={\n",
    "    \"dims\": 1536,\n",
    "    \"embed\": init_embeddings(\"openai:text-embedding-3-small\"),\n",
    "    \"fields\": [\"text\"],\n",
    "})\n",
    "\n",
    "# Namespace for the memory to save\n",
    "user_id = \"1\"\n",
    "namespace_for_memory = (user_id, \"memories\")\n",
    "\n",
    "# Save a memory to namespace as key and value\n",
    "key = str(uuid.uuid4())\n",
    "value = {\"text\": memory_collection.memories[0].content}\n",
    "in_memory_embed_store.put(namespace_for_memory, key, value)\n",
    "\n",
    "key = str(uuid.uuid4())\n",
    "value = {\"text\": memory_collection.memories[1].content}\n",
    "in_memory_embed_store.put(namespace_for_memory, key, value)"
   ],
   "outputs": [],
   "execution_count": 117
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T20:31:16.217686Z",
     "start_time": "2025-03-23T20:31:15.945749Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for m in in_memory_embed_store.search(namespace_for_memory, query=\"Lance\"):\n",
    "    print(m.dict())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'namespace': ['1', 'memories'], 'key': '8faa1d20-c98b-4e01-9c37-8af6d66c43a4', 'value': {'text': \"User's name is Lance.\"}, 'created_at': '2025-03-23T20:31:13.440318+00:00', 'updated_at': '2025-03-23T20:31:13.440331+00:00', 'score': 0.7283749541421611}\n",
      "{'namespace': ['1', 'memories'], 'key': '813556b3-4e9f-4083-9a29-3da2e556ca02', 'value': {'text': 'User likes to bike.'}, 'created_at': '2025-03-23T20:31:13.748853+00:00', 'updated_at': '2025-03-23T20:31:13.748870+00:00', 'score': 0.2700927546801956}\n"
     ]
    }
   ],
   "execution_count": 118
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Graph with memory collection"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T20:31:23.445597Z",
     "start_time": "2025-03-23T20:31:23.429604Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize the model\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)"
   ],
   "outputs": [],
   "execution_count": 119
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T20:31:24.839716Z",
     "start_time": "2025-03-23T20:31:24.818284Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trustcall_extractor = create_extractor(\n",
    "    llm,\n",
    "    tools=[Memory],\n",
    "    tool_choice=\"Memory\",\n",
    "    enable_inserts=True, # This allows the extractor to insert new memories\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 120
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T20:32:42.792336Z",
     "start_time": "2025-03-23T20:32:42.785725Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.messages import merge_message_runs\n",
    "from langgraph.store.base import BaseStore\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from IPython.display import Image, display\n",
    "from langgraph.graph import MessagesState\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "MODEL_SYSTEM_MESSAGE = \"\"\"\n",
    "You are a helpful chatbot. You are designed to be a companion to a user.\n",
    "You have a long term memory which keeps track of information you learn about the user over time.\n",
    "Current Memory (may include updated memories from this conversation):\n",
    "{memory}\n",
    "\"\"\"\n",
    "\n",
    "TRUSTCALL_INSTRUCTION = \"\"\"\n",
    "Reflect on following interaction.\n",
    "Use the provided tools to retain any necessary memories about the user.\n",
    "Use parallel tool calling to handle updates and insertions simultaneously:\n",
    "\"\"\"\n",
    "\n",
    "# Node\n",
    "def call_llm(state: MessagesState, config: RunnableConfig, store: BaseStore):\n",
    "    # Get the user ID from the config\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "\n",
    "    # Retrieve memory from the store\n",
    "    namespace = (\"memories\", user_id)\n",
    "    memories = store.search(namespace)\n",
    "\n",
    "    # Format the memories for the system prompt\n",
    "    info = \"\\n\".join(f\"- {mem.value['content']}\" for mem in memories)\n",
    "    system_msg = MODEL_SYSTEM_MESSAGE.format(memory=info)\n",
    "\n",
    "    response = llm.invoke([SystemMessage(content=system_msg)] + state[\"messages\"])\n",
    "\n",
    "    return {\"messages\": response}\n",
    "\n",
    "def write_memory(state: MessagesState, config: RunnableConfig, store: BaseStore):\n",
    "    # Get the user ID from the config\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "\n",
    "    # Define the namespace for the memories\n",
    "    namespace = (\"memories\", user_id)\n",
    "\n",
    "    # Retrieve the most recent memories for context\n",
    "    existing_items = store.search(namespace)\n",
    "\n",
    "    # Format the existing memories for the Trustcall extractor\n",
    "    tool_name = \"Memory\"\n",
    "    existing_memories = ([(existing_item.key, tool_name, existing_item.value)\n",
    "                          for existing_item in existing_items]\n",
    "                          if existing_items\n",
    "                          else None\n",
    "                        )\n",
    "\n",
    "    # Merge the chat history and the instruction\n",
    "    updated_messages=list(merge_message_runs(messages=[SystemMessage(content=TRUSTCALL_INSTRUCTION)] + state[\"messages\"]))\n",
    "\n",
    "    # Invoke the extractor\n",
    "    result = trustcall_extractor.invoke({\"messages\": updated_messages, \"existing\": existing_memories})\n",
    "\n",
    "    # Save the memories from Trustcall to the store\n",
    "    for r, rmeta in zip(result[\"responses\"], result[\"response_metadata\"]):\n",
    "        store.put(namespace,\n",
    "                  rmeta.get(\"json_doc_id\", str(uuid.uuid4())),\n",
    "                  r.model_dump(mode=\"json\"),\n",
    "            )"
   ],
   "outputs": [],
   "execution_count": 129
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T20:32:43.399208Z",
     "start_time": "2025-03-23T20:32:43.351482Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"call_llm\", call_llm)\n",
    "builder.add_node(\"write_memory\", write_memory)\n",
    "\n",
    "builder.add_edge(START, \"call_llm\")\n",
    "builder.add_edge(\"call_llm\", \"write_memory\")\n",
    "builder.add_edge(\"write_memory\", END)\n",
    "\n",
    "# Store for long-term (across-thread) memory\n",
    "across_thread_memory = InMemoryStore()\n",
    "\n",
    "# Checkpointer for short-term (within-thread) memory\n",
    "within_thread_memory = MemorySaver()\n",
    "\n",
    "# Compile the graph with the checkpointer fir and store\n",
    "graph = builder.compile(checkpointer=within_thread_memory, store=across_thread_memory)\n",
    "\n",
    "# View\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ],
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 130
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T20:32:45.374190Z",
     "start_time": "2025-03-23T20:32:43.970588Z"
    }
   },
   "source": [
    "# We supply a thread ID for short-term (within-thread) memory\n",
    "# We supply a user ID for long-term (across-thread) memory \n",
    "config = {\"configurable\": {\"thread_id\": \"1\", \"user_id\": \"1\"}}\n",
    "\n",
    "# User input \n",
    "input_messages = [HumanMessage(content=\"Hi, my name is Lance\")]\n",
    "\n",
    "result = graph.invoke({\"messages\": input_messages}, config)\n",
    "# Run the graph\n",
    "for m in result['messages']:\n",
    "    m.pretty_print()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001B[1m Human Message \u001B[0m=================================\n",
      "\n",
      "Hi, my name is Lance\n",
      "==================================\u001B[1m Ai Message \u001B[0m==================================\n",
      "\n",
      "Hi Lance! It's great to meet you. How can I assist you today?\n"
     ]
    }
   ],
   "execution_count": 131
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T20:32:47.953203Z",
     "start_time": "2025-03-23T20:32:45.434109Z"
    }
   },
   "source": [
    "# User input \n",
    "input_messages = [HumanMessage(content=\"I like to bike around San Francisco\")]\n",
    "\n",
    "# Run the graph\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001B[1m Human Message \u001B[0m=================================\n",
      "\n",
      "I like to bike around San Francisco\n",
      "==================================\u001B[1m Ai Message \u001B[0m==================================\n",
      "\n",
      "That sounds like a lot of fun! San Francisco has some beautiful routes for biking. Do you have a favorite trail or area you like to explore?\n"
     ]
    }
   ],
   "execution_count": 132
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T20:32:48.982437Z",
     "start_time": "2025-03-23T20:32:48.979089Z"
    }
   },
   "source": [
    "# Namespace for the memory to save\n",
    "user_id = \"1\"\n",
    "namespace = (\"memories\", user_id)\n",
    "memories = across_thread_memory.search(namespace)\n",
    "for m in memories:\n",
    "    print(m.dict())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'namespace': ['memories', '1'], 'key': '26260a0b-99b1-492b-852d-64dc95785236', 'value': {'content': 'User likes to bike around San Francisco.'}, 'created_at': '2025-03-23T20:32:47.950864+00:00', 'updated_at': '2025-03-23T20:32:47.950865+00:00', 'score': None}\n",
      "{'namespace': ['memories', '1'], 'key': '694e6126-08be-429b-933f-da806ddda196', 'value': {'content': 'User likes to bike around San Francisco.'}, 'created_at': '2025-03-23T20:32:47.950839+00:00', 'updated_at': '2025-03-23T20:32:47.950841+00:00', 'score': None}\n"
     ]
    }
   ],
   "execution_count": 133
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T20:32:53.479391Z",
     "start_time": "2025-03-23T20:32:50.623961Z"
    }
   },
   "source": [
    "# User input \n",
    "input_messages = [HumanMessage(content=\"I also enjoy going to bakeries\")]\n",
    "\n",
    "# Run the graph\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001B[1m Human Message \u001B[0m=================================\n",
      "\n",
      "I also enjoy going to bakeries\n",
      "==================================\u001B[1m Ai Message \u001B[0m==================================\n",
      "\n",
      "Biking and bakeries make a great combination! Do you have a favorite bakery in San Francisco, or are you still exploring different ones?\n"
     ]
    }
   ],
   "execution_count": 134
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continue the conversation in a new thread."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T20:33:01.908392Z",
     "start_time": "2025-03-23T20:32:57.210555Z"
    }
   },
   "source": [
    "# We supply a thread ID for short-term (within-thread) memory\n",
    "# We supply a user ID for long-term (across-thread) memory \n",
    "config = {\"configurable\": {\"thread_id\": \"2\", \"user_id\": \"1\"}}\n",
    "\n",
    "# User input \n",
    "input_messages = [HumanMessage(content=\"What bakeries do you recommend for me?\")]\n",
    "\n",
    "# Run the graph\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001B[1m Human Message \u001B[0m=================================\n",
      "\n",
      "What bakeries do you recommend for me?\n",
      "==================================\u001B[1m Ai Message \u001B[0m==================================\n",
      "\n",
      "Since you enjoy biking around San Francisco, you might want to check out some of these bakeries that are perfect for a bike ride stop:\n",
      "\n",
      "1. **Tartine Bakery** - Located in the Mission District, it's famous for its bread and pastries. It's a great spot to refuel after a ride.\n",
      "\n",
      "2. **Arsicault Bakery** - Known for its incredible croissants, this bakery in the Richmond District is a must-visit.\n",
      "\n",
      "3. **B. Patisserie** - Situated in Lower Pacific Heights, it offers a delightful selection of pastries and is a nice spot to relax.\n",
      "\n",
      "4. **Mr. Holmes Bakehouse** - In the Tenderloin, this bakery is famous for its cruffins and other creative pastries.\n",
      "\n",
      "5. **Noe Valley Bakery** - A cozy spot in Noe Valley, perfect for a quick stop during your biking adventures.\n",
      "\n",
      "Do any of these sound like a good fit for your next ride?\n"
     ]
    }
   ],
   "execution_count": 135
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T20:33:13.741307Z",
     "start_time": "2025-03-23T20:33:13.739157Z"
    }
   },
   "source": "# Q: Why we need both in-thread and in-memory storage? What is the difference?",
   "outputs": [],
   "execution_count": 136
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Task\n",
    "Modify the graph to add only the most relevant memory according to the last message"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
